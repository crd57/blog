# 初始化

## 输入数据预处理

进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练/验证/测试集，那么这个做法是错误的。**应该怎么做呢？应该先分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值。** 

## 权重初始化

### 小随机数初始化

如果神经元刚开始的时候是随机且不相等的，那么它们将计算出不同的更新，并将自身变成整个网络的不同部分。小随机数权重初始化的实现方法是：**W = 0.01 \* np.random.randn(D,H)。**其中**randn**函数是基于零均值和标准差的一个高斯分布来生成随机数的。根据这个式子，每个神经元的权重向量都被初始化为一个随机向量，而这些随机向量又服从一个多变量高斯分布，这样在输入空间中，所有的神经元的指向是随机的。也可以使用均匀分布生成的随机数，但是从实践结果来看，对于算法的结果影响极小。 

**警告**并不是小数值一定会得到好的结果。例如，一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度（因为梯度与权重值是成比例的）。这就会很大程度上减小反向传播中的“梯度信号”，在深度网络中，就会出现问题。

### 使用1/sqrt(n)校准方差

### Dropout

在**predict**函数中不进行随机失活，但是对于两个隐层的输出都要乘以![p](https://www.zhihu.com/equation?tex=p)，调整其数值范围。这一点非常重要，因为在测试时所有的神经元都能看见它们的输入，因此我们想要神经元的输出与训练时的预期输出是一致的。 

想要经过Dropout后输出的分布和期望的分布一致，就要对输出的结果进行改造。

以![p=0.5](https://www.zhihu.com/equation?tex=p%3D0.5)为例，在测试时神经元必须把它们的输出减半，这是因为在训练的时候它们的输出只有一半。 

先假设有一个神经元![x](https://www.zhihu.com/equation?tex=x)的输出，那么进行随机失活的时候，该神经元的输出就是![px+(1-p)0](https://www.zhihu.com/equation?tex=px%2B%281-p%290)，这是有![1-p](https://www.zhihu.com/equation?tex=1-p)的概率神经元的输出为0。在测试时神经元总是激活的，就必须调整![x\to px](https://www.zhihu.com/equation?tex=x%5Cto+px)来保持同样的预期输出。 



###实践

通过交叉验证获得一个全局使用的L2正则化强度是比较常见的。在使用L2正则化的同时在所有层后面使用随机失活也很常见。值一般默认设为0.5，也可能在验证集上调参。 、



## 损失函数

有两个部分组成：

1. 数据损失 对所有样本的数据损失求平均
2. 正则化 控制神经网络的容量来防止其过拟合 

#### 问题：

**类别数目巨大。** 当标签集非常庞大（例如字典中的所有英语单词，或者ImageNet中的22000种分类），就需要使用*分层Softmax（**Hierarchical Softmax**）*了（[参考文献](https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1310.4546.pdf)）。分层softmax将标签分解成一个树。每个标签都表示成这个树上的一个路径，这个树的每个节点处都训练一个Softmax分类器来在左和右分枝之间做决策。树的结构对于算法的最终结果影响很大，而且一般需要具体问题具体分析。 

**属性（Attribute）分类。**上面两个损失公式的前提，都是假设每个样本只有一个正确的标签![y_i](https://www.zhihu.com/equation?tex=y_i)。但是如果![y_i](https://www.zhihu.com/equation?tex=y_i)是一个二值向量，每个样本可能有，也可能没有某个属性，而且属性之间并不相互排斥呢？比如在Instagram上的图片，就可以看成是被一个巨大的标签集合中的某个子集打上标签，一张图片上可能有多个标签。在这种情况下，一个明智的方法是为每个属性创建一个独立的二分类的分类器。 



L2损失比起较为稳定的Softmax损失来，其最优化过程要困难很多。直观而言，它需要网络具备一个特别的性质，即对于每个输入（和增量）都要输出一个确切的正确值。而在Softmax中就不是这样，每个评分的准确值并不是那么重要：只有当它们量级适当的时候，才有意义。还有，L2损失鲁棒性不好，因为异常值可以导致很大的梯度。所以在面对一个回归问题时，先考虑将输出变成二值化是否真的不够用。 