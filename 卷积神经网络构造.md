# 卷积神经网络

## 如何建立神经网络

###激活函数 

#### Sigmoid

$$
\sigma (x) = \frac{1}{1+e^{-x}}
$$

![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/600px-Logistic-curve.svg.png)

##### 问题：

- 饱和神经元使梯度消失

- 也不是以零为中心的

- 指数运算计算代价太大
#####不以零为中心的影响


一开始使用均值为0的初始化避免下降的时候更新速率太慢，只沿一个方向进行
$$
f(\vec x; \vec w, b) = f\bigl(w_0x_0 + w_1x_1 + b\bigr).
$$
现在假设，参数 $w_0$ , $w_1$ 的最优解 $w_0^{*}$ , $w_1^{*}$ 满足条件
$$
\begin{cases}w_0 < w_0^{*}, \\ w_1\geqslant w_1^{*}.\end{cases}
$$
这也就是说，我们希望 $w_0$ 适当增大，但希望 $w_1$ 适当减小。考虑到上一小节提到的更新方向的问题，这就必然要求 $x_0$ 和 $x_1$ 符号相反。

但在 Sigmoid 函数中，输出值恒为正。这也就是说， 如果上一级神经元采用 Sigmoid 函数作为激活函数，那么我们无法做到 $x_0$ 和 $x_1$ 符号相反 。此时，模型为了收敛，不得不向逆风前行的风助力帆船一样，走 Z 字形逼近最优解。

![](https://liam0205.me/uploads/images/MachineLearning/zig-zag-gradient.png)

如图，模型参数走绿色箭头能够最快收敛，但由于输入值的符号总是为正，所以模型参数可能走类似红色折线的箭头。如此一来，使用 Sigmoid 函数作为激活函数的神经网络，收敛速度就会慢上不少了。

#### tanh激活函数

$$
\tanh \left( x\right) =2\sigma \left( 2x\right) -1
$$

![](http://mathworld.wolfram.com/images/interactive/TanhReal.gif)



##### 问题：

当饱和时依然出现梯度消失

#### ReLU

$$
f\left( x\right) =\max \left( 0,x\right) 
$$

![](http://cs231n.github.io/assets/nn1/relu.jpeg)



##### 问题：

- 不以0为中心输出
- Dead ReLU(在负数时梯度消失) ,合理设置学习率，会降低这种情况的发生概率。

使用较小的正偏置来初始化，以增加它在初始化时被激活的可能性，并获得一些更新

#### Leaky ReLU

$$
f(x) = \max(0.01x,x)
$$

![](http://lamda.nju.edu.cn/weixs/project/CNNTricks/imgs/leaky.png)

#### PReLU

$$
f(x) = \max(\alpha x,x)
$$

将$\alpha$当作一个可以反向传播和学习的参数

#### ELU

$$
f(x)=\lambda 
\begin{cases}
\alpha(e^x-1) & x \le 0 \\
x & x>0
\end{cases}
$$

$$
\begin{equation}
f'(x)=
\begin{cases}
f(x)+\alpha, & \text{$x\leq 0$} \\
1, & \text{$x\gt 0$}
\end{cases}
\end{equation}
$$

![](https://i.stack.imgur.com/h7BrH.png)

争议性观点：

可能这种模型会对噪音更具有鲁棒性

#### Maxout

$$
\max(w_1^Tx+b_1,w_2^Tx+b2)
$$

##### 问题：

会将权重矩阵翻倍

### 经验法则

1. 优先使用ReLU,但是要注意学习的速率
2. 也可以尝试Leaky ReLU/PReLU/ELU(实用性较弱)
3. 可以尝试tanh，但是不要期望太高
4. 一般不要使用sigmoid



### 预处理

#### 零均值化

若输入的数据都是正的，那么得到的权重也全都是正的。会导致梯度下降走Z字型路线，得到的基本上是此最优的优化。

方法：

- 减去整张图像的均值（AlexNet)
- 减去每一个通道的均值(VGGNet)

#### 归一化

所有的特征都在相同的值域内，并且这样特征贡献相同

### 权重初始化

**本来我们希望不同的结点学习到不同的参数，但是由于参数相同以及输出值都一样，不同的结点根本无法学到不同的特征！这样就失去了网络学习特征的意义了。**

#### 非常小的随机数

在小型网络中适用，打破了参数对称问题。但是在大型网络中，会导致梯度弥散。

#### Xavier初始化

```python
W = np.random.randon(fan_in, fan_out)/np.sqrt(fan_in)
```

但是使用ReLU激活，它会杀死“一半”的系数，不会得到正确的方差。

可以试着使用以下公式解决

```python
W = np.random.randon(fan_in, fan_out)/np.sqrt(fan_in/2)
```



### 批归一化

在我们想要的高斯范围内保持激活

观察每一层的输入，并计算均值和方差然后进行归一化

假设一批次的样本有N个，维度为D，每一维单独计算均值和方差



### 梯度检查



## 训练中的动态变化

### 监控学习过程

1. 数据预处理
2. 初始化我们的网络
3. 从小数据集开始训练
4. 调整学习率，$[e^{-5},e^{-3}]$之间

### 参数更新

### 超参数优化

执行交叉验证

## 模型评估

### 模型集成

